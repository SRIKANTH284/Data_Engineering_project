



# Translating Batch Processing to Stream Processing in Big Data Analytics

This project demonstrates how to **convert traditional batch processing code** into **stream processing code** automatically, using a custom AST-based converter.

You will learn how to run both **batch mode** and **streaming mode** locally on your machine!

---

##  Getting Started

Follow these simple steps to set up and run the project on your local machine.

---

##  Step 1: Clone the Repository

```bash
git clone https://github.com/SRIKANTH284/Data_Engineering_project.git 
```



---

##  Step 2: Open the Project in VS Code

- Open the cloned repository folder inside **Visual Studio Code** (or your preferred code editor).

---

##  Step 3: Delete Old Output Files

Before running anything:
- **Delete** the `batch_output` folder  
  *(Reason: A new `batch_output/` folder will be freshly created when you run `batch.py`.)*
- **Delete** the existing `stream.py` file  
  *(Reason: A new `stream.py` will be auto-generated by `ast_converter.py`.)*

---

##  Step 4: Enter into Project Directory

```bash
cd Data_Engineering_project
```

 Now you are inside the main project folder.

---

##  Step 5: Install Required Python Libraries

```bash
pip3 install -r requirements.txt
```

This will install necessary packages like `pyspark`, `astor`, etc.

---

##  Step 6: Run the Batch Processing Code

```bash
python3 batch.py
```

 After running `batch.py`, you will see:
- A new folder named `batch_output/` created.
- This contains the batch-processed output based on the given input `data.csv`.

---

##  Step 7: Generate the Stream Processing Code

Now run:

```bash
python3 ast_converter.py batch.py
```

 This means:
- We are passing `batch.py` as input to `ast_converter.py`.
- It will automatically generate a new file called `stream.py` inside the project.

---

##  Step 8: Verify the Generated Stream Code

Finally, run the generated streaming code:

```bash
python3 stream.py
```

---

##  Step 9: Copy CSVs from the project directory into the stream input directory to simulate stream data 

Finally, open a new terminal and run this command to copy CSVs

```bash
cp data1.csv stream_input/data1.csv
cp data2.csv stream_input/data2.csv
```

 This will:
- Start reading the CSV files inside `stream_input/` folder **one by one**.
- Apply the same transformation logic (filtering and grouping).
- Output results **directly on the console** batch-by-batch.
- 

---
  
#  Notes

- The output may vary slightly per batch because streaming processes **one file at a time**, applying filters on each file separately.
- After all files are processed, the final cumulative results will match the batch processing results!

---

# Project Structure

```
Data_Engineering_project/
├── batch_input/
│   └── data.csv
├── batch_output/   (auto-generated after running batch.py)
├── stream_input/   (copy data1.csv and data2.csv into this directory to simulate stream data)
├── batch.py
├── data1.csv
├── data2.csv
├── ast_converter.py
├── stream.py  (auto-generated after running ast_converter.py)
├── requirements.txt
├── README.md
```

---

# Conclusion

This project shows how **Batch Processing** can be **transformed into Stream Processing** using Python automation and Spark Structured Streaming!

 Easy to run locally  
 Understands both batch and stream workflows  
 Great for learning Big Data pipeline transformation

